{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_filename = \"crimes_2016.csv\"\n",
    "dataset = pd.read_csv(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Turn date column into datetime and just get the month\n",
    "#dates = pd.to_datetime(dataset['Date'], format=\"%m/%d/%Y %H:%M:%S PM\")\n",
    "def convert_time(dates):\n",
    "    time = []\n",
    "    for i in range(dates.shape[0]):\n",
    "        time.append(dates.iloc[i].split(\" \")[0])\n",
    "        \n",
    "    return time\n",
    "\n",
    "\n",
    "def get_hours(dates):\n",
    "    hours = []\n",
    "    for i in range(dates.shape[0]):\n",
    "        h = dates.iloc[i].split(\" \")\n",
    "        \n",
    "        hour = int(h[1].split(\":\")[0])\n",
    "          \n",
    "        hours.append(hour)\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-01-28 00:53:00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Date'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = convert_time(dataset['Date'])\n",
    "hours = get_hours(dataset['Date'])\n",
    "dataset['Month'] = pd.DatetimeIndex(dataset['Date']).month\n",
    "dataset['Hour'] = pd.to_numeric(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Drop noisy variables and redundant variables: description, updated on, beat, year \n",
    "dataset = dataset.drop('Description', 1)\n",
    "dataset = dataset.drop('Beat', 1)\n",
    "dataset = dataset.drop('Year', 1) ## all in 2016\n",
    "dataset = dataset.drop('Date', 1) ## only using month \n",
    "dataset = dataset.drop('Location', 1) ## already have lat and long as separate fields\n",
    "dataset = dataset.drop('Block', 1) \n",
    "\n",
    "## Drop UICR because it's not independent of what we are trying to predict\n",
    "dataset = dataset.drop('IUCR', 1)\n",
    "dataset = dataset.drop('FBI Code', 1) ## not independent variable\n",
    "\n",
    "# Drop na values. We will use missing rows in other analysis\n",
    "dataset = dataset.dropna(axis=0, how='any')\n",
    "\n",
    "## Uncomment if you'd like to filter to keep only theft and assault\n",
    "dataset = dataset[(dataset['Primary Type'] == 'THEFT') | (dataset['Primary Type'] == 'ASSAULT')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Location Description</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Domestic</th>\n",
       "      <th>District</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Community Area</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Month</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>THEFT</td>\n",
       "      <td>RESIDENCE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>41.755939</td>\n",
       "      <td>-87.608986</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>THEFT</td>\n",
       "      <td>GAS STATION</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>41.735931</td>\n",
       "      <td>-87.653642</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>THEFT</td>\n",
       "      <td>GAS STATION</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>41.779998</td>\n",
       "      <td>-87.629295</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>THEFT</td>\n",
       "      <td>GAS STATION</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>41.772201</td>\n",
       "      <td>-87.702981</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>THEFT</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.868034</td>\n",
       "      <td>-87.639215</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Primary Type Location Description Arrest Domestic  District  Ward  \\\n",
       "10        THEFT            RESIDENCE  False     True       6.0   6.0   \n",
       "11        THEFT          GAS STATION  False    False      22.0  21.0   \n",
       "12        THEFT          GAS STATION  False    False       3.0  20.0   \n",
       "13        THEFT          GAS STATION   True    False       8.0  15.0   \n",
       "14        THEFT           RESTAURANT   True    False       1.0   2.0   \n",
       "\n",
       "    Community Area   Latitude  Longitude  Month  Hour  \n",
       "10            69.0  41.755939 -87.608986      1     0  \n",
       "11            71.0  41.735931 -87.653642      1     1  \n",
       "12            68.0  41.779998 -87.629295      1     1  \n",
       "13            66.0  41.772201 -87.702981      1     0  \n",
       "14            28.0  41.868034 -87.639215      1     2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = dataset.iloc[:, 0:2]\n",
    "X2 = dataset.iloc[:, 3:]\n",
    "X = pd.concat([X1, X2], axis=1, join='inner')\n",
    "X = X.values\n",
    "y = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Encode Categorical variables \n",
    "primary_enc = LabelEncoder()\n",
    "location_enc = LabelEncoder()\n",
    "domestic_enc = LabelEncoder()\n",
    "\n",
    "X[:, 0] = primary_enc.fit_transform(X[:, 0]) \n",
    "X[:, 1] = location_enc.fit_transform(X[:, 1]) \n",
    "X[:, 2] = domestic_enc.fit_transform(X[:, 2])\n",
    "\n",
    "#Create dummy variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0, 1, 2, 3, 4, 5])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = None)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 variables explain 0.90 variance\n",
      "172 variables explain 0.90 variance\n",
      "173 variables explain 0.91 variance\n",
      "174 variables explain 0.91 variance\n",
      "175 variables explain 0.91 variance\n",
      "176 variables explain 0.92 variance\n",
      "177 variables explain 0.92 variance\n",
      "178 variables explain 0.92 variance\n",
      "179 variables explain 0.93 variance\n",
      "180 variables explain 0.93 variance\n",
      "181 variables explain 0.93 variance\n",
      "182 variables explain 0.94 variance\n",
      "183 variables explain 0.94 variance\n",
      "184 variables explain 0.94 variance\n",
      "185 variables explain 0.95 variance\n",
      "186 variables explain 0.95 variance\n",
      "187 variables explain 0.95 variance\n",
      "188 variables explain 0.95 variance\n",
      "189 variables explain 0.96 variance\n",
      "190 variables explain 0.96 variance\n",
      "191 variables explain 0.96 variance\n",
      "192 variables explain 0.96 variance\n",
      "193 variables explain 0.96 variance\n",
      "194 variables explain 0.97 variance\n",
      "195 variables explain 0.97 variance\n",
      "196 variables explain 0.97 variance\n",
      "197 variables explain 0.97 variance\n",
      "198 variables explain 0.97 variance\n",
      "199 variables explain 0.97 variance\n",
      "200 variables explain 0.98 variance\n",
      "201 variables explain 0.98 variance\n",
      "202 variables explain 0.98 variance\n",
      "203 variables explain 0.98 variance\n",
      "204 variables explain 0.98 variance\n",
      "205 variables explain 0.98 variance\n",
      "206 variables explain 0.98 variance\n",
      "207 variables explain 0.98 variance\n",
      "208 variables explain 0.98 variance\n",
      "209 variables explain 0.98 variance\n",
      "210 variables explain 0.99 variance\n",
      "211 variables explain 0.99 variance\n",
      "212 variables explain 0.99 variance\n",
      "213 variables explain 0.99 variance\n",
      "214 variables explain 0.99 variance\n",
      "215 variables explain 0.99 variance\n",
      "216 variables explain 0.99 variance\n",
      "217 variables explain 0.99 variance\n",
      "218 variables explain 0.99 variance\n",
      "219 variables explain 0.99 variance\n",
      "220 variables explain 0.99 variance\n",
      "221 variables explain 0.99 variance\n",
      "222 variables explain 0.99 variance\n",
      "223 variables explain 0.99 variance\n",
      "224 variables explain 1.00 variance\n",
      "225 variables explain 1.00 variance\n",
      "226 variables explain 1.00 variance\n",
      "227 variables explain 1.00 variance\n",
      "228 variables explain 1.00 variance\n",
      "229 variables explain 1.00 variance\n",
      "230 variables explain 1.00 variance\n",
      "231 variables explain 1.00 variance\n",
      "232 variables explain 1.00 variance\n",
      "233 variables explain 1.00 variance\n",
      "234 variables explain 1.00 variance\n",
      "235 variables explain 1.00 variance\n",
      "236 variables explain 1.00 variance\n",
      "237 variables explain 1.00 variance\n",
      "238 variables explain 1.00 variance\n",
      "239 variables explain 1.00 variance\n",
      "240 variables explain 1.00 variance\n",
      "241 variables explain 1.00 variance\n",
      "242 variables explain 1.00 variance\n",
      "243 variables explain 1.00 variance\n",
      "244 variables explain 1.00 variance\n",
      "245 variables explain 1.00 variance\n",
      "246 variables explain 1.00 variance\n",
      "247 variables explain 1.00 variance\n",
      "248 variables explain 1.00 variance\n",
      "249 variables explain 1.00 variance\n",
      "250 variables explain 1.00 variance\n",
      "251 variables explain 1.00 variance\n",
      "252 variables explain 1.00 variance\n",
      "253 variables explain 1.00 variance\n",
      "254 variables explain 1.00 variance\n",
      "255 variables explain 1.00 variance\n",
      "256 variables explain 1.00 variance\n"
     ]
    }
   ],
   "source": [
    "# Get a sense of how much variance is explained by the number of variables\n",
    "def variance_explained(variances, start_printing_at):   \n",
    "    result = 0\n",
    "    for i in range(len(variances)):\n",
    "        result += variances[i]\n",
    "        if (result > start_printing_at):\n",
    "            print(\"{} variables explain {:.2f} variance\".format(i, result))\n",
    "\n",
    "\n",
    "variance_explained(explained_variance, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We decide to use 194 variables that explain 97% of the variance\n",
    "pca = PCA(n_components = 194)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict test results\n",
    "#y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accuracy = (cm[0][0] + cm[1][1])/ np.sum(cm)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create ensemble with AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt = DecisionTreeClassifier() \n",
    "logit = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "nb = GaussianNB()\n",
    "svc= SVC(kernel = 'rbf')\n",
    "\n",
    "\n",
    "#Using Decision Tree\n",
    "clf = AdaBoostClassifier(n_estimators=50, base_estimator=dt, learning_rate=1)\n",
    "fit=clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Logistic Regresssion\n",
    "clf = AdaBoostClassifier(n_estimators=50, base_estimator=logit, learning_rate=1)\n",
    "fit=clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Naive Bayes\n",
    "clf = AdaBoostClassifier(n_estimators=50, base_estimator=nb, learning_rate=1)\n",
    "fit=clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using Support Vector Machine\n",
    "clf = AdaBoostClassifier(n_estimators=50, base_estimator=svc, algorithm='SAMME', learning_rate=1)\n",
    "fit=clf.fit(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
